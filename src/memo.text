問題点

summary_log: 出力の平均値を計算して csv ファイルに保存するかどうか。を実行するために奮闘中
    ファイル参照のエラーがでる
    例　data_processing.py line15

    data_processing.py line7の意味がわからない

    だいたい出てくるエラー
    AttributeError: type object 'DataFrame' has no attribute 'read_csv'
    おそらく、csvファイルまで参照できていないためこのようなエラーが出る。

    read_csv→read_csv

新しいバージョンを使おう
    自分の中に影響があるため

    新しい環境にあったコードに書き換えること


    どうしてもの場合
    https://aws.amazon.com/jp/sagemaker/ 


ucbの式に則ってrsの式を入れること。
いい感じの成績が出なかった場合は、TSの方に融合させる形でやろうと思ってる。

比較すべきアルゴリズム、
UCBのscoreの部分
score = ks[i] * (means(期待値)ーaleph_opt) 　　meansを平均と分散の正規分布に変えてみたり
aleph_opt = (1.0 + 0.9）/2

1.0=一番高い期待値の腕　0.9=二番目に高い期待値の腕

基準値設定を学習しながら得られるようにする。


それぞれの正規分布ずを足して2で割った部分を計算

今年中にやりましょう。すぐ！

RSがどれほど分散に弱いかを検討したい
他の方法も提案しながら。

報酬環境によっての変化
分散の荒さとか

聞く足がかり

RSがもたらした応用

動画によって

ーーーーーー
とりあえずはUSBを改変
次にUCBを変えてRSにする


アブストラクトメモ

gaussian bandit

バンディット問題の先行研究の様子
    先行研究の多くは報酬をベルヌーイ分布と仮定しているが、実世界でのタスクは報酬を実数値として用いたいことが多い。
    本研究では、確率的バンディット問題においての、（報酬を実数値にした際の）最適なアルゴリズムを考察する。（示す。）
            　（報酬が平均・分散が未知の場合と仮定した場合の）
            （報酬スケールが異なる場合の分布から生成される場合の）

バンディット問題とは、

バンディット問題の多くはベルヌーイ分布（1か0）からの報酬を仮定している。
実際には報酬を実数にしたい場合が多く存在する。
    例　動画や音楽のレコメンドにおいて CTR (クリック率) ではなく再生時間など実数を報酬にしたい場合。
        なぜそうするか：動画の場合のクリック率は、サムネイルやタイトルが大きな影響を及ぼしている。
                    ：再生時間を報酬とすると、動画の内容の良し悪しや、サムネイルやタイトルから視聴者が求めていたものかどうかを判別できる。
                    判別することによって、各ユーザに適した動画を提供することができる。




キーワード

強化学習の基本的な問題であるバンディット問題は、インターネット広告配信やゲーム木探索などに幅広く応用されている。

実世界でのタスクの多くは、報酬を実数値として取り扱わなければならない問題が発生している。

環境と相互作用しながら行動を学習する強化学習。



